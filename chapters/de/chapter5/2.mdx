# Was, wenn mein Datensatz nicht auf dem Hub ist?[[was-wenn-mein-datensatz-nicht-auf-dem-hub-ist]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section2.ipynb"},
]} />

Du wei√üt, wie du den [Hugging Face Hub](https://huggingface.co/datasets) nutzen kannst, um Datens√§tze herunterzuladen. Aber oft wirst du mit Daten arbeiten, die entweder auf deinem Laptop oder auf einem entfernten Server gespeichert sind. In diesem Abschnitt zeigen wir dir, wie ü§ó Datasets verwendet werden kann, um Datens√§tze zu laden, die nicht auf dem Hugging Face Hub verf√ºgbar sind.

<Youtube id="HyQgpJTkRdE"/>

## Arbeiten mit lokalen und entfernten Datens√§tzen[[arbeiten-mit-lokalen-und-entfernten-datens√§tzen]]

ü§ó Datasets bietet Ladeskripte zum Laden lokaler und entfernter Datens√§tze. Es unterst√ºtzt mehrere g√§ngige Datenformate, wie zum Beispiel:

|    Datenformat     | Ladeskript |                         Beispiel                          |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset("csv", data_files="meine_datei.csv")`     |
|    Textdateien     |     `text`     |    `load_dataset("text", data_files="meine_datei.txt")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset("json", data_files="meine_datei.jsonl")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset("pandas", data_files="mein_dataframe.pkl")` |

Wie in der Tabelle gezeigt, m√ºssen wir f√ºr jedes Datenformat nur den Typ des Ladeskripts in der `load_dataset()`-Funktion angeben, zusammen mit einem `data_files`-Argument, das den Pfad zu einer oder mehreren Dateien angibt. Beginnen wir damit, einen Datensatz aus lokalen Dateien zu laden; sp√§ter werden wir sehen, wie man dasselbe mit entfernten Dateien macht.

## Laden eines lokalen Datensatzes[[laden-eines-lokalen-datensatzes]]

F√ºr dieses Beispiel verwenden wir den [SQuAD-it-Datensatz](https://github.com/crux82/squad-it/), einen umfangreichen Datensatz f√ºr Fragenbeantwortung auf Italienisch.

Die Trainings- und Test-Splits werden auf GitHub gehostet, sodass wir sie mit einem einfachen `wget`-Befehl herunterladen k√∂nnen:

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

Dadurch werden zwei komprimierte Dateien namens *SQuAD_it-train.json.gz* und *SQuAD_it-test.json.gz* heruntergeladen, die wir mit dem Linux-Befehl `gzip` dekomprimieren k√∂nnen:

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```bash
SQuAD_it-test.json.gz:	   87.4% -- ersetzt durch SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- ersetzt durch SQuAD_it-train.json
```

Wir sehen, dass die komprimierten Dateien durch _SQuAD_it-train.json_ und _SQuAD_it-test.json_ ersetzt wurden und dass die Daten im JSON-Format gespeichert sind.

<Tip>

‚úé Wenn du dich fragst, warum in den obigen Shell-Befehlen ein `!`-Zeichen steht: Das liegt daran, dass wir sie in einem Jupyter Notebook ausf√ºhren. Entferne einfach das Pr√§fix, wenn du den Datensatz in einem Terminal herunterladen und entpacken m√∂chtest.

</Tip>

Um eine JSON-Datei mit der `load_dataset()`-Funktion zu laden, m√ºssen wir nur wissen, ob es sich um gew√∂hnliches JSON (√§hnlich einem verschachtelten Dictionary) oder JSON Lines (zeilengetrenntes JSON) handelt. Wie viele Datens√§tze zur Fragenbeantwortung verwendet SQuAD-it das verschachtelte Format, wobei der gesamte Text in einem `data`-Feld gespeichert ist. Das bedeutet, wir k√∂nnen den Datensatz laden, indem wir das `field`-Argument wie folgt angeben:

```py
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

Standardm√§√üig erstellt das Laden lokaler Dateien ein `DatasetDict`-Objekt mit einem `train`-Split. Wir k√∂nnen dies sehen, indem wir das `squad_it_dataset`-Objekt inspizieren:

```py
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

Dies zeigt uns die Anzahl der Zeilen und die Spaltennamen, die dem Trainingssatz zugeordnet sind. Wir k√∂nnen eines der Beispiele anzeigen, indem wir wie folgt in den `train`-Split indizieren:

```py
squad_it_dataset["train"][0]
```

```python out
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si √® verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

Gro√üartig, wir haben unseren ersten lokalen Datensatz geladen! Aber w√§hrend dies f√ºr den Trainingssatz funktionierte, m√∂chten wir eigentlich sowohl den `train`- als auch den `test`-Split in einem einzigen `DatasetDict`-Objekt haben, damit wir `Dataset.map()`-Funktionen auf beide Splits gleichzeitig anwenden k√∂nnen. Dazu k√∂nnen wir dem `data_files`-Argument ein Dictionary √ºbergeben, das jeden Split-Namen einer Datei zuordnet, die diesem Split zugeordnet ist:

```py
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python out
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

Das ist genau das, was wir wollten. Jetzt k√∂nnen wir verschiedene Vorverarbeitungstechniken anwenden, um die Daten zu bereinigen, die Bewertungen zu tokenisieren und so weiter.

<Tip>

Das `data_files`-Argument der `load_dataset()`-Funktion ist ziemlich flexibel und kann entweder ein einzelner Dateipfad, eine Liste von Dateipfaden oder ein Dictionary sein, das Split-Namen auf Dateipfade abbildet. Du kannst auch Dateien nach einem bestimmten Muster gem√§√ü den Regeln der Unix-Shell ausw√§hlen (z. B. kannst du alle JSON-Dateien in einem Verzeichnis als einen einzigen Split ausw√§hlen, indem du `data_files="*.json"` setzt). Weitere Details findest du in der ü§ó Datasets-[Dokumentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files).

</Tip>

Die Ladeskripte in ü§ó Datasets unterst√ºtzen tats√§chlich die automatische Dekomprimierung der Eingabedateien, sodass wir die Verwendung von `gzip` h√§tten √ºberspringen k√∂nnen, indem wir das `data_files`-Argument direkt auf die komprimierten Dateien verweisen:

```py
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Dies kann n√ºtzlich sein, wenn du nicht viele GZIP-Dateien manuell dekomprimieren m√∂chtest. Die automatische Dekomprimierung gilt auch f√ºr andere g√§ngige Formate wie ZIP und TAR. Du musst also nur `data_files` auf die komprimierten Dateien verweisen, und schon bist du fertig!

Nachdem du nun wei√üt, wie du lokale Dateien auf deinem Laptop oder Desktop laden kannst, schauen wir uns das Laden von entfernten Dateien an.

## Laden eines entfernten Datensatzes[[laden-eines-entfernten-datensatzes]]

Wenn du als Data Scientist oder Programmierer in einem Unternehmen arbeitest, ist es wahrscheinlich, dass die Datens√§tze, die du analysieren m√∂chtest, auf einem entfernten Server gespeichert sind. Gl√ºcklicherweise ist das Laden von entfernten Dateien genauso einfach wie das Laden lokaler Dateien! Anstatt einen Pfad zu lokalen Dateien anzugeben, verweisen wir das `data_files`-Argument von `load_dataset()` auf eine oder mehrere URLs, unter denen die entfernten Dateien gespeichert sind. F√ºr den auf GitHub gehosteten SQuAD-it-Datensatz k√∂nnen wir beispielsweise `data_files` einfach auf die _SQuAD_it-*.json.gz_-URLs verweisen:

```py
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

Dies gibt dasselbe `DatasetDict`-Objekt zur√ºck, das wir oben erhalten haben, erspart uns aber den Schritt des manuellen Herunterladens und Dekomprimierens der _SQuAD_it-*.json.gz_-Dateien. Damit schlie√üen wir unseren Exkurs in die verschiedenen M√∂glichkeiten zum Laden von Datens√§tzen ab, die nicht auf dem Hugging Face Hub gehostet werden. Nachdem wir nun einen Datensatz zum Spielen haben, machen wir uns an verschiedene Datenmanipulationstechniken!

<Tip>

‚úèÔ∏è **Probier es aus!** W√§hle einen anderen Datensatz aus, der auf GitHub oder im [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) gehostet wird, und versuche, ihn sowohl lokal als auch remote mit den oben vorgestellten Techniken zu laden. F√ºr Bonuspunkte versuche, einen Datensatz zu laden, der im CSV- oder Textformat gespeichert ist (siehe die [Dokumentation](https://huggingface.co/docs/datasets/loading#local-and-remote-files) f√ºr weitere Informationen zu diesen Formaten).

</Tip> 