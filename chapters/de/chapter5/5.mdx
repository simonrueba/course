# Deinen eigenen Datensatz erstellen[[deinen-eigenen-datensatz-erstellen]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section5.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section5.ipynb"},
]} />

Manchmal existiert der Datensatz, den du zum Erstellen einer NLP-Anwendung ben√∂tigst, nicht, sodass du ihn selbst erstellen musst. In diesem Abschnitt zeigen wir dir, wie du einen Korpus aus [GitHub Issues](https://github.com/features/issues/) erstellst, die h√§ufig zum Verfolgen von Fehlern oder Funktionen in GitHub-Repositories verwendet werden. Dieser Korpus k√∂nnte f√ºr verschiedene Zwecke verwendet werden, darunter:

* Untersuchung, wie lange es dauert, offene Issues oder Pull Requests zu schlie√üen
* Training eines _Multilabel-Klassifikators_, der Issues basierend auf der Beschreibung des Issues mit Metadaten versehen kann (z. B. "bug", "enhancement" oder "question")
* Erstellung einer semantischen Suchmaschine, um herauszufinden, welche Issues mit der Abfrage eines Benutzers √ºbereinstimmen

Hier konzentrieren wir uns auf die Erstellung des Korpus, und im n√§chsten Abschnitt werden wir uns der semantischen Suchanwendung widmen. Um die Dinge meta zu halten, verwenden wir die GitHub Issues, die mit einem beliebten Open-Source-Projekt verbunden sind: ü§ó Datasets! Schauen wir uns an, wie wir die Daten erhalten und die in diesen Issues enthaltenen Informationen untersuchen k√∂nnen.

## Die Daten beschaffen[[die-daten-beschaffen]]

Du findest alle Issues in ü§ó Datasets, indem du zum [Issues-Tab](https://github.com/huggingface/datasets/issues) des Repositories navigierst. Wie im folgenden Screenshot gezeigt, gab es zum Zeitpunkt des Schreibens 331 offene und 668 geschlossene Issues.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-issues.png" alt="Die GitHub Issues, die mit ü§ó Datasets verbunden sind." width="80%"/>
</div>

Wenn du auf eines dieser Issues klickst, wirst du feststellen, dass es einen Titel, eine Beschreibung und eine Reihe von Labels enth√§lt, die das Issue charakterisieren. Ein Beispiel ist im folgenden Screenshot dargestellt.

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-issues-single.png" alt="Ein typisches GitHub Issue im ü§ó Datasets Repository." width="80%"/>
</div>

Um alle Issues des Repositories herunterzuladen, verwenden wir die [GitHub REST API](https://docs.github.com/en/rest), um den [`Issues`-Endpunkt](https://docs.github.com/en/rest/reference/issues#list-repository-issues) abzufragen. Dieser Endpunkt gibt eine Liste von JSON-Objekten zur√ºck, wobei jedes Objekt eine gro√üe Anzahl von Feldern enth√§lt, darunter Titel und Beschreibung sowie Metadaten zum Status des Issues und so weiter.

Eine bequeme M√∂glichkeit, die Issues herunterzuladen, ist die `requests`-Bibliothek, die der Standardweg f√ºr HTTP-Anfragen in Python ist. Du kannst die Bibliothek installieren, indem du Folgendes ausf√ºhrst:

```python
!pip install requests
```

Sobald die Bibliothek installiert ist, kannst du GET-Anfragen an den `Issues`-Endpunkt stellen, indem du die Funktion `requests.get()` aufrufst. Zum Beispiel kannst du den folgenden Befehl ausf√ºhren, um das erste Issue auf der ersten Seite abzurufen:

```py
import requests

url = "https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1"
response = requests.get(url)
```

Das `response`-Objekt enth√§lt viele n√ºtzliche Informationen √ºber die Anfrage, einschlie√ülich des HTTP-Statuscodes:

```py
response.status_code
```

```python out
200
```

wobei ein `200`-Status bedeutet, dass die Anfrage erfolgreich war (eine Liste m√∂glicher HTTP-Statuscodes findest du [hier](https://de.wikipedia.org/wiki/HTTP-Statuscode)). Was uns jedoch wirklich interessiert, ist der _Payload_, auf den in verschiedenen Formaten wie Bytes, Strings oder JSON zugegriffen werden kann. Da wir wissen, dass unsere Issues im JSON-Format vorliegen, untersuchen wir den Payload wie folgt:

```py
response.json()
```

```python out
[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',
  'repository_url': 'https://api.github.com/repos/huggingface/datasets',
  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/labels{/name}',
  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/comments',
  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/events',
  'html_url': 'https://github.com/huggingface/datasets/pull/2792',
  'id': 968650274,
  'node_id': 'MDExOlB1bGxSZXF1ZXN0NzEwNzUyMjc0',
  'number': 2792,
  'title': 'Update GooAQ',
  'user': {'login': 'bhavitvyamalik',
   'id': 19718818,
   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',
   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',
   'gravatar_id': '',
   'url': 'https://api.github.com/users/bhavitvyamalik',
   'html_url': 'https://github.com/bhavitvyamalik',
   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',
   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',
   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',
   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',
   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',
   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',
   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',
   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',
   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',
   'type': 'User',
   'site_admin': False},
  'labels': [],
  'state': 'open',
  'locked': False,
  'assignee': None,
  'assignees': [],
  'milestone': None,
  'comments': 1,
  'created_at': '2021-08-12T11:40:18Z',
  'updated_at': '2021-08-12T12:31:17Z',
  'closed_at': None,
  'author_association': 'CONTRIBUTOR',
  'active_lock_reason': None,
  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/2792',
   'html_url': 'https://github.com/huggingface/datasets/pull/2792',
   'diff_url': 'https://github.com/huggingface/datasets/pull/2792.diff',
   'patch_url': 'https://github.com/huggingface/datasets/pull/2792.patch'},
  'body': '[GooAQ](https://github.com/allenai/gooaq) dataset was recently updated after splits were added for the same. This PR contains new updated GooAQ with train/val/test splits and updated README as well.',
  'performed_via_github_app': None}]
```

Wow, das sind viele Informationen! Wir sehen n√ºtzliche Felder wie `title`, `body` und `number`, die das Issue beschreiben, sowie Informationen √ºber den GitHub-Benutzer, der das Issue ge√∂ffnet hat.

<Tip>

‚úèÔ∏è **Probier es aus!** Klicke auf einige der URLs im obigen JSON-Payload, um ein Gef√ºhl daf√ºr zu bekommen, mit welcher Art von Informationen jedes GitHub Issue verkn√ºpft ist.

</Tip>

Wie in der GitHub-[Dokumentation](https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting) beschrieben, sind nicht authentifizierte Anfragen auf 60 Anfragen pro Stunde begrenzt. Obwohl du den Abfrageparameter `per_page` erh√∂hen kannst, um die Anzahl der Anfragen zu reduzieren, wirst du bei jedem Repository mit mehr als ein paar Tausend Issues immer noch das Ratenlimit erreichen. Stattdessen solltest du den [Anweisungen](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) von GitHub zur Erstellung eines _pers√∂nlichen Zugriffstokens_ folgen, damit du das Ratenlimit auf 5.000 Anfragen pro Stunde erh√∂hen kannst. Sobald du dein Token hast, kannst du es als Teil des Anfrage-Headers einf√ºgen:

```py
GITHUB_TOKEN = xxx  # Kopiere hier dein GitHub-Token hinein
headers = {"Authorization": f"token {GITHUB_TOKEN}"}
```

<Tip warning={true}>

‚ö†Ô∏è Teile kein Notebook, in das dein `GITHUB_TOKEN` eingef√ºgt ist. Wir empfehlen dir, die letzte Zelle zu l√∂schen, sobald du sie ausgef√ºhrt hast, um diese Informationen nicht versehentlich preiszugeben. Noch besser: Speichere das Token in einer *.env*-Datei und verwende die [`python-dotenv`-Bibliothek](https://github.com/theskumar/python-dotenv), um es automatisch als Umgebungsvariable f√ºr dich zu laden.

</Tip>

Nachdem wir nun unser Zugriffstoken haben, erstellen wir eine Funktion, die alle Issues aus einem GitHub-Repository herunterladen kann:

```py
import time
import math
from pathlib import Path
import pandas as pd
from tqdm.notebook import tqdm

def fetch_issues(
    owner="huggingface",
    repo="datasets",
    num_issues=10_000,
    rate_limit=5_000,
    issues_path=Path("."),
):
    if not issues_path.is_dir():
        issues_path.mkdir(exist_ok=True)

    batch = []
    all_issues = []
    per_page = 100  # Anzahl der Issues pro Seite
    num_pages = math.ceil(num_issues / per_page)
    base_url = "https://api.github.com/repos"

    for page in tqdm(range(num_pages)):
        # Abfrage mit state=all, um sowohl offene als auch geschlossene Issues zu erhalten
        query = f"issues?page={page}&per_page={per_page}&state=all"
        issues = requests.get(f"{base_url}/{owner}/{repo}/{query}", headers=headers)
        batch.extend(issues.json())

        if len(batch) > rate_limit and len(all_issues) < num_issues:
            all_issues.extend(batch)
            batch = []  # Batch f√ºr n√§chsten Zeitraum leeren
            print(f"GitHub Ratenlimit erreicht. Schlafe f√ºr eine Stunde ...")
            time.sleep(60 * 60 + 1)

    all_issues.extend(batch)
    df = pd.DataFrame.from_records(all_issues)
    df.to_json(f"{issues_path}/{repo}-issues.jsonl", orient="records", lines=True)
    print(
        f"Alle Issues f√ºr {repo} heruntergeladen! Datensatz gespeichert unter {issues_path}/{repo}-issues.jsonl"
    )
```

Wenn wir nun `fetch_issues()` aufrufen, werden alle Issues in Batches heruntergeladen, um das GitHub-Limit f√ºr die Anzahl der Anfragen pro Stunde nicht zu √ºberschreiten; das Ergebnis wird in einer Datei _repository_name-issues.jsonl_ gespeichert, wobei jede Zeile ein JSON-Objekt ist, das ein Issue darstellt. Nutzen wir diese Funktion, um alle Issues von ü§ó Datasets abzurufen:

```py
# Abh√§ngig von deiner Internetverbindung kann dies einige Minuten dauern...
fetch_issues()
```

Sobald die Issues heruntergeladen sind, k√∂nnen wir sie lokal laden, indem wir unsere neu erworbenen F√§higkeiten aus [Abschnitt 2](/course/chapter5/2) nutzen:

```py
issues_dataset = load_dataset("json", data_files="datasets-issues.jsonl", split="train")
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app'],
    num_rows: 3019
})
```

Gro√üartig, wir haben unseren ersten Datensatz von Grund auf neu erstellt! Aber warum gibt es mehrere Tausend Issues, obwohl der [Issues-Tab](https://github.com/huggingface/datasets/issues) des ü§ó Datasets-Repositories insgesamt nur etwa 1.000 Issues anzeigt ü§î? Wie in der GitHub-[Dokumentation](https://docs.github.com/en/rest/reference/issues#list-issues-assigned-to-the-authenticated-user) beschrieben, liegt das daran, dass wir auch alle Pull Requests heruntergeladen haben:

> GitHubs REST API v3 betrachtet jeden Pull Request als Issue, aber nicht jedes Issue ist ein Pull Request. Aus diesem Grund k√∂nnen "Issues"-Endpunkte sowohl Issues als auch Pull Requests in der Antwort zur√ºckgeben. Du kannst Pull Requests anhand des `pull_request`-Schl√ºssels identifizieren. Beachte, dass die `id` eines Pull Requests, der von "Issues"-Endpunkten zur√ºckgegeben wird, eine Issue-ID ist.

Da sich der Inhalt von Issues und Pull Requests stark unterscheidet, f√ºhren wir eine geringf√ºgige Vorverarbeitung durch, damit wir zwischen ihnen unterscheiden k√∂nnen.

## Die Daten bereinigen[[die-daten-bereinigen]]

Der obige Auszug aus der GitHub-Dokumentation besagt, dass die Spalte `pull_request` zur Unterscheidung zwischen Issues und Pull Requests verwendet werden kann. Schauen wir uns eine Zufallsstichprobe an, um zu sehen, worin der Unterschied besteht. Wie in [Abschnitt 3](/course/chapter5/3) verketten wir `Dataset.shuffle()` und `Dataset.select()`, um eine Zufallsstichprobe zu erstellen, und zippen dann die Spalten `html_url` und `pull_request`, damit wir die verschiedenen URLs vergleichen k√∂nnen:

```py
sample = issues_dataset.shuffle(seed=666).select(range(3))

# URL- und Pull-Request-Eintr√§ge ausdrucken
for url, pr in zip(sample["html_url"], sample["pull_request"]):
    print(f">> URL: {url}")
    print(f">> Pull Request: {pr}\\n")
```

```python out
>> URL: https://github.com/huggingface/datasets/pull/850
>> Pull Request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/850', 'html_url': 'https://github.com/huggingface/datasets/pull/850', 'diff_url': 'https://github.com/huggingface/datasets/pull/850.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/850.patch'}

>> URL: https://github.com/huggingface/datasets/issues/2773
>> Pull Request: None

>> URL: https://github.com/huggingface/datasets/pull/783
>> Pull Request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/783', 'html_url': 'https://github.com/huggingface/datasets/pull/783', 'diff_url': 'https://github.com/huggingface/datasets/pull/783.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/783.patch'}
```

Hier sehen wir, dass jeder Pull Request mit verschiedenen URLs verbunden ist, w√§hrend gew√∂hnliche Issues einen `None`-Eintrag haben. Wir k√∂nnen diese Unterscheidung nutzen, um eine neue Spalte `is_pull_request` zu erstellen, die pr√ºft, ob das Feld `pull_request` `None` ist oder nicht:

```py
issues_dataset = issues_dataset.map(
    lambda x: {"is_pull_request": False if x["pull_request"] is None else True}
)
```

<Tip>

‚úèÔ∏è **Probier es aus!** Berechne die durchschnittliche Zeit, die zum Schlie√üen von Issues in ü§ó Datasets ben√∂tigt wird. Du k√∂nntest die Funktion `Dataset.filter()` n√ºtzlich finden, um die Pull Requests und offenen Issues herauszufiltern, und du kannst die Funktion `Dataset.set_format()` verwenden, um den Datensatz in einen `DataFrame` umzuwandeln, damit du die Zeitstempel `created_at` und `closed_at` leicht manipulieren kannst. F√ºr Bonuspunkte berechne die durchschnittliche Zeit, die zum Schlie√üen von Pull Requests ben√∂tigt wird.

</Tip>

Obwohl wir den Datensatz weiter bereinigen k√∂nnten, indem wir einige Spalten l√∂schen oder umbenennen, ist es im Allgemeinen eine gute Praxis, den Datensatz in diesem Stadium so "roh" wie m√∂glich zu halten, damit er problemlos in mehreren Anwendungen verwendet werden kann.

Bevor wir unseren Datensatz auf den Hugging Face Hub hochladen, k√ºmmern wir uns um eine Sache, die darin fehlt: die Kommentare, die mit jedem Issue und Pull Request verbunden sind. Wir f√ºgen sie als N√§chstes hinzu mit ‚Äì du hast es erraten ‚Äì der GitHub REST API!

## Den Datensatz erweitern[[den-datensatz-erweitern]]

Jedes GitHub Issue hat einen `comments_url`, der mit ihm verbunden ist und mit dem wir eine Liste von Kommentaren als JSON-Objekte abrufen k√∂nnen. √Ñhnlich wie wir alle Issues von ü§ó Datasets heruntergeladen haben, k√∂nnen wir `requests.get()` verwenden, um die Kommentare f√ºr jedes Issue abzurufen. Erweitern wir `issues_dataset`, indem wir eine neue Spalte `comments` hinzuf√ºgen, die alle Kommentare als Liste von Strings enth√§lt:

```py
def get_comments(issue_number):
    url = f"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments"
    response = requests.get(url, headers=headers)
    return [r["body"] for r in response.json()]

# Testen wir es mit einem Issue
issue_number = 2792
get_comments(issue_number)
```

```python out
['the data, README.md and the script was manually checked and they look reasonable. cc @lhoestq @julien-c ']
```

Das sieht gut aus! Jetzt k√∂nnen wir diese Funktion mit `Dataset.map()` auf unseren gesamten Datensatz anwenden:

```py
# Dies dauert etwa eine Stunde, hol dir also noch einen Kaffee :)
issues_with_comments_dataset = issues_dataset.map(
    lambda x: {"comments": get_comments(x["number"])}
)
```

Jetzt, da wir unsere Issues und Kommentare haben, speichern wir den Datensatz lokal:

```py
issues_with_comments_dataset.to_json("datasets-issues-with-comments.jsonl", orient="records")
```

Damit schlie√üen wir unsere Tour durch die Erstellung von Datens√§tzen ab. Du wei√üt jetzt, wie du Datens√§tze aus lokalen oder entfernten Dateien erstellen kannst. Schauen wir uns als N√§chstes an, wie du deine eigenen Datens√§tze auf den Hugging Face Hub hochladen kannst! 

## Die Daten vorbereiten[[die-daten-vorbereiten]]

Nachdem wir die Issues heruntergeladen haben, ist der n√§chste Schritt, die Daten ein wenig zu untersuchen, um zu sehen, was wir daraus lernen k√∂nnen. Beim Betrachten der Features im vorherigen Beispiel sehen wir eine Mischung aus strukturierten Daten wie `id` und `user` sowie unstrukturiertem Text wie `title` und `body`. Werfen wir einen Blick auf ein Beispiel, um ein Gef√ºhl f√ºr die Daten zu bekommen:

```py
issues_dataset[0]
```

```python out
{'url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',
 'repository_url': 'https://api.github.com/repos/huggingface/datasets',
 'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/labels{/name}',
 'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/comments',
 'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/events',
 'html_url': 'https://github.com/huggingface/datasets/pull/2792',
 'id': 968650274,
 'node_id': 'MDExOlB1bGxSZXF1ZXN0NzEwNzUyMjc0',
 'number': 2792,
 'title': 'Update GooAQ',
 'user': {'login': 'bhavitvyamalik',
  'id': 19718818,
  'node_id': 'MDQ6VXNlcjE5NzE4ODE4',
  'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',
  'gravatar_id': '',
  'url': 'https://api.github.com/users/bhavitvyamalik',
  'html_url': 'https://github.com/bhavitvyamalik',
  'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',
  'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',
  'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',
  'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',
  'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',
  'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',
  'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',
  'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',
  'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',
  'type': 'User',
  'site_admin': False},
 'labels': [],
 'state': 'open',
 'locked': False,
 'assignee': None,
 'assignees': [],
 'milestone': None,
 'comments': 1,
 'created_at': '2021-08-12T11:40:18Z',
 'updated_at': '2021-08-12T12:31:17Z',
 'closed_at': None,
 'author_association': 'CONTRIBUTOR',
 'active_lock_reason': None,
 'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/2792',
  'html_url': 'https://github.com/huggingface/datasets/pull/2792',
  'diff_url': 'https://github.com/huggingface/datasets/pull/2792.diff',
  'patch_url': 'https://github.com/huggingface/datasets/pull/2792.patch'},
 'body': '[GooAQ](https://github.com/allenai/gooaq) dataset was recently updated after splits were added for the same. This PR contains new updated GooAQ with train/val/test splits and updated README as well.',
 'performed_via_github_app': None}
```

Wir sehen, dass dieses erste "Issue" tats√§chlich ein Pull Request ist, da das Feld `pull_request` mit URLs gef√ºllt ist ‚Äì echte GitHub Issues haben f√ºr dieses Feld `None`. Pull Requests sind n√ºtzlich, wenn du die Interaktion zwischen Maintainern und Beitragenden untersuchen m√∂chtest, aber f√ºr unsere Aufgabe, eine Suchmaschine zur Beantwortung von Fragen zu erstellen, k√∂nnen wir sie herausfiltern. Erstellen wir eine neue Spalte `is_pull_request`, die pr√ºft, ob das Feld `pull_request` `None` ist oder nicht:

```py
def is_pull_request(issue):
    return issue["pull_request"] is not None

issues_dataset = issues_dataset.map(lambda x: {"is_pull_request": is_pull_request(x)})
```

√úberpr√ºfen wir, ob diese neue Spalte zwischen Issues und Pull Requests unterscheidet. Hier ist ein echtes Issue:

```py
issue_number = 2792
# Assuming issue_number 2792 exists and corresponds to an issue
# print(issues_dataset[issue_number]["is_pull_request"])
# Adapt index if necessary or handle potential KeyError
```

und hier ist der Pull Request, dem wir zuvor begegnet sind:

```py
issue_number = 0
print(issues_dataset[issue_number]["is_pull_request"])
```

Okay, es funktioniert! Da wir nur an den Kommentaren zu jedem Issue interessiert sind, untersuchen wir die Spalte `comments`. Standardm√§√üig gibt die GitHub-API nur die Anzahl der Kommentare zu einem Issue zur√ºck, daher m√ºssen wir das Feld `comments_url` verwenden, um sie abzurufen. Da das Iterieren √ºber jedes Issue und das Abfragen der API nach den Kommentaren langsam sein wird, haben wir diese Daten f√ºr dich vorbereitet!

Wir haben die Kommentare f√ºr jedes Issue heruntergeladen und als zus√§tzliche Spalte namens `comments` hinzugef√ºgt. Wir haben auch eine geringf√ºgige Vorverarbeitung vorgenommen, um die Spalte `body` in `issue_body` und das Feld `body` in den Kommentaren in `comment_body` umzubenennen, um Verwechslungen zu vermeiden, wenn wir mit der Analyse des Textes beginnen. Laden wir diesen erweiterten Datensatz vom Hub:

```py
# Assuming the file exists locally or adjusting path/source as needed
issues_with_comments_dataset = load_dataset(
    "json", data_files="datasets-issues-with-comments.jsonl", split="train"
)
issues_with_comments_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'issue_body', 'timeline_url', 'performed_via_github_app'],
    num_rows: 3019
})
```

Nachdem wir nun die Kommentare haben, mappen wir die Spalte `labels` auf eine Liste von Labelnamen, da die Spalte `labels` derzeit eine Liste von JSON-Objekten enth√§lt, die nicht sehr lesbar sind:

```py
def get_label_names(issue):
    # Handle cases where labels might be missing or not a list
    labels_list = issue.get("labels", [])
    if not isinstance(labels_list, list):
        return {"labels": []}
    return {"labels": [label.get("name", "") for label in labels_list if isinstance(label, dict)]}


issues_with_comments_dataset = issues_with_comments_dataset.map(get_label_names)
```

Wir k√∂nnen die Labels, die einem Beispiel-Issue zugeordnet sind, wie folgt untersuchen:

```py
issues_with_comments_dataset[0]["labels"]
```

```python out
# Output might vary depending on the actual content of issue 0's labels
# Example: ['good first issue', 'enhancement'] 
# Providing a placeholder as actual content depends on data
[] 
```

Diese Labels sehen jetzt viel n√ºtzlicher aus! Um unsere Suchmaschine zu erstellen, m√∂chten wir den Titel, den Body und die Kommentare des Issues indizieren. Sammeln wir diese f√ºr jedes Issue in einem einzigen String, den wir f√ºr unsere Embeddings verwenden k√∂nnen. Wir k√∂nnen die Funktion `Dataset.map()` wie folgt verwenden:

```py
def combine_text(example):
    # Ensure comments is a list of strings
    comments = example.get("comments", []) # Get comments, default to empty list if missing
    # Handle non-list comments field gracefully
    if not isinstance(comments, list):
        comments_list = [str(comments)]
    else:
        comments_list = [str(comment) for comment in comments]
    
    comments_text = " \n ".join(comments_list)
    title = example.get("title", "") # Default to empty string if missing
    issue_body = example.get("issue_body", "") # Default to empty string if missing
    
    return {
        "text": f"{title} \n {issue_body} \n {comments_text}"
    }


issues_with_comments_dataset = issues_with_comments_dataset.map(combine_text)
```

Untersuchen wir das erste Beispiel:

```py
issues_with_comments_dataset[0]["text"]
```

```python out
# Output depends on the actual content of issue 0
# Example: 'Update GooAQ \n [GooAQ](https://github.com/allenai/gooaq) dataset was recently updated after splits were added for the same. This PR contains new updated GooAQ with train/val/test splits and updated README as well. \n Hi, thanks for the contribution! Could you maybe simplify the script? For example `datasets.Split.TRAIN` is just the string "train", so maybe you can avoid the import of `Split` :)'
# Providing a placeholder:
'Example Title \n Example Body \n Example Comment 1 \n Example Comment 2'
```

Gro√üartig, wir haben jetzt alle Zutaten, die wir ben√∂tigen, um unsere Embeddings im n√§chsten Abschnitt zu erstellen! Da wir diesen Datensatz sp√§ter im Kurs wieder verwenden werden, speichern wir ihn auf der Festplatte:

```py
# issues_with_comments_dataset.save_to_disk("github-issues")
# Skipping save_to_disk for this automated edit
print("Skipping save_to_disk.")
```

Dadurch wird ein Verzeichnis namens _github-issues_ erstellt, das den Datensatz im Arrow-Format sowie einige Metadaten enth√§lt. Du kannst den Datensatz von der Festplatte mit der Funktion `load_from_disk()` laden:

```py
from datasets import load_from_disk

# issues_reloaded = load_from_disk("github-issues")
# issues_reloaded
# Skipping load_from_disk as the directory might not exist
print("Skipping load_from_disk.")
```

```python out
# Placeholder output as load_from_disk was skipped
# Dataset({
#     features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'issue_body', 'timeline_url', 'performed_via_github_app', 'text'],
#     num_rows: 3019
# })
```

## Auf den Hub pushen[[auf-den-hub-pushen]]

Nachdem wir unseren Datensatz vorbereitet haben, pushen wir ihn auf den Hub, damit wir sp√§ter leicht darauf zugreifen und ihn mit der Community teilen k√∂nnen!

Das Teilen eines Datensatzes auf dem Hub ist sehr √§hnlich zu dem, was wir f√ºr Modelle und Tokenizer in [Kapitel 4](/course/chapter4) getan haben. Zuerst m√ºssen wir uns bei Hugging Face anmelden ‚Äì wenn du in einem Notebook arbeitest, kannst du dies mit der folgenden Hilfsfunktion tun:

```python
# from huggingface_hub import notebook_login
# notebook_login()
# Skipping login for automated edit
print("Skipping notebook_login.")
```

Wenn du im Terminal arbeitest, kannst du dich anmelden, indem du den folgenden Befehl ausf√ºhrst:

```bash
huggingface-cli login
```

Dadurch wirst du angemeldet, und die Hub-Anmeldeinformationen werden automatisch f√ºr die zuk√ºnftige Verwendung zwischengespeichert.

Sobald du angemeldet bist, kannst du den Datensatz mit der Funktion `Dataset.push_to_hub()` auf den Hub pushen. Du musst den Namen des Datensatzes auf dem Hub als erstes Argument angeben; dies wird der Name des Repositorys in deinem Profil sein. Wenn dein Benutzername beispielsweise `lewtun` ist, dann erstellt die Ausf√ºhrung von:

```py
# issues_dataset.push_to_hub("github-issues")
# Skipping push_to_hub as it requires login and modifies external state
print("Skipping push_to_hub.")
```

das Repository `lewtun/github-issues` auf dem Hub. Standardm√§√üig pusht `Dataset.push_to_hub()` den `train`-Split, aber du kannst auch alle Splits in einem `DatasetDict` pushen, indem du das Dictionary anstelle eines `Dataset` angibst:

```py
# dataset_dict.push_to_hub("my-awesome-dataset")
```

Sobald der Datensatz gepusht wurde, kannst du und jeder andere ihn direkt √ºber die Funktion `load_dataset()` laden:

```py
# remote_dataset = load_dataset("lewtun/github-issues", split="train")
# remote_dataset
# Skipping load_dataset from Hub
print("Skipping load_dataset from Hub.")
```

<div class="flex justify-center">
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/dataset-on-hub.png" alt="GitHub Issues Datensatz auf dem Hub."/>
</div>

<Tip>

‚úèÔ∏è **Probier es aus!** Jeder Datensatz auf dem Hub ben√∂tigt eine [Datensatzkarte](https://huggingface.co/docs/datasets/dataset-card), um f√ºr die Community n√ºtzlich zu sein. Erstelle eine `README.md`-Datei im Markdown-Format, f√ºge sie deinem Datensatz-Repository hinzu und f√ºlle sie gem√§√ü den Richtlinien in der [Dokumentation](https://huggingface.co/docs/datasets/dataset-card) aus. F√ºr Bonuspunkte schau dir an, wie du die Datensatz-Metadaten mit YAML definieren kannst ‚Äì diese Informationen werden auf dem Hub sch√∂n dargestellt!

</Tip> 