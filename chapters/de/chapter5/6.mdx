<FrameworkSwitchCourse {fw} />

# Semantische Suche mit FAISS[[semantische-suche-mit-faiss]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section6_tf.ipynb"},
]} />

{/if}

In [Abschnitt 5](/course/chapter5/5) haben wir einen Datensatz aus GitHub Issues und Kommentaren aus dem ü§ó Datasets-Repository erstellt. In diesem Abschnitt werden wir diese Informationen nutzen, um eine Suchmaschine zu bauen, die uns helfen kann, Antworten auf unsere dringendsten Fragen zur Bibliothek zu finden!

<Youtube id="OATCgQtNX2o"/>

## Embeddings f√ºr die semantische Suche verwenden[[embeddings-f√ºr-die-semantische-suche-verwenden]]

Wie wir in [Kapitel 1](/course/chapter1) gesehen haben, stellen Transformer-basierte Sprachmodelle jedes Token in einem Textabschnitt als _Embedding-Vektor_ dar. Es stellt sich heraus, dass man die einzelnen Embeddings "poolen" kann, um eine Vektordarstellung f√ºr ganze S√§tze, Abs√§tze oder (in einigen F√§llen) Dokumente zu erstellen. Diese Embeddings k√∂nnen dann verwendet werden, um √§hnliche Dokumente im Korpus zu finden, indem die Punktprodukt-√Ñhnlichkeit (oder eine andere √Ñhnlichkeitsmetrik) zwischen jedem Embedding berechnet und die Dokumente mit der gr√∂√üten √úberlappung zur√ºckgegeben werden.

In diesem Abschnitt werden wir Embeddings verwenden, um eine semantische Suchmaschine zu entwickeln. Diese Suchmaschinen bieten mehrere Vorteile gegen√ºber herk√∂mmlichen Ans√§tzen, die auf dem Abgleich von Schl√ºsselw√∂rtern in einer Abfrage mit den Dokumenten basieren.

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search.svg" alt="Semantische Suche."/>
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/semantic-search-dark.svg" alt="Semantische Suche."/>
</div>

## Laden und Vorbereiten des Datensatzes[[laden-und-vorbereiten-des-datensatzes]]

Als Erstes m√ºssen wir unseren Datensatz mit GitHub Issues herunterladen, also verwenden wir wie gewohnt die Funktion `load_dataset()`:

```py
from datasets import load_dataset

issues_dataset = load_dataset("lewtun/github-issues", split="train")
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

Hier haben wir den standardm√§√üigen `train`-Split in `load_dataset()` angegeben, sodass ein `Dataset` anstelle eines `DatasetDict` zur√ºckgegeben wird. Als Erstes m√ºssen wir die Pull Requests herausfiltern, da diese in der Regel selten zur Beantwortung von Benutzeranfragen verwendet werden und Rauschen in unserer Suchmaschine verursachen. Wie inzwischen bekannt sein sollte, k√∂nnen wir die Funktion `Dataset.filter()` verwenden, um diese Zeilen in unserem Datensatz auszuschlie√üen. Wenn wir schon dabei sind, filtern wir auch Zeilen ohne Kommentare heraus, da diese keine Antworten auf Benutzeranfragen liefern:

```py
issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python out
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})
```

Wir sehen, dass unser Datensatz viele Spalten enth√§lt, von denen wir die meisten nicht ben√∂tigen, um unsere Suchmaschine zu bauen. Aus Suchperspektive sind die informativsten Spalten `title`, `body` und `comments`, w√§hrend `html_url` uns einen Link zur√ºck zum urspr√ºnglichen Issue liefert. Verwenden wir die Funktion `Dataset.remove_columns()`, um den Rest zu l√∂schen:

```py
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})
```

Um unsere Embeddings zu erstellen, erg√§nzen wir jeden Kommentar um den Titel und den Textk√∂rper des Issues, da diese Felder oft n√ºtzliche Kontextinformationen enthalten. Da unsere `comments`-Spalte derzeit eine Liste von Kommentaren f√ºr jedes Issue ist, m√ºssen wir die Spalte "explodieren", sodass jede Zeile aus einem `(html_url, title, body, comment)`-Tupel besteht. In Pandas k√∂nnen wir dies mit der [`DataFrame.explode()`-Funktion](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html) tun, die f√ºr jedes Element in einer listen√§hnlichen Spalte eine neue Zeile erstellt und dabei alle anderen Spaltenwerte repliziert. Um dies in Aktion zu sehen, wechseln wir zun√§chst zum Pandas `DataFrame`-Format:

```py
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

Wenn wir die erste Zeile in diesem `DataFrame` untersuchen, sehen wir, dass diesem Issue vier Kommentare zugeordnet sind:

```py
df["comments"][0].tolist()
```

```python out
['the bug code locate in Ôºö\\r\\n    if data_args.task_name is not None:\\r\\n        # Downloading and loading a dataset from the hub.\\r\\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\\r\\n\\r\\nNormally, it should work if you wait a little and then retry.\\r\\n\\r\\nCould you please confirm if the problem persists?',
 'cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

Wenn wir `df` explodieren, erwarten wir eine Zeile f√ºr jeden dieser Kommentare. Pr√ºfen wir, ob das der Fall ist:

```py
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>html_url</th>
      <th>title</th>
      <th>comments</th>
      <th>body</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>the bug code locate in Ôºö\\r\\n    if data_args.task_name is not None...</td>
      <td>Hello,\\r\\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>Hi @jinec,\\r\\n\\r\\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
      <td>Hello,\\r\\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>cannot connectÔºåeven by Web browserÔºåplease check that  there is some  problems„ÄÇ</td>
      <td>Hello,\\r\\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
      <td>Hello,\\r\\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
  </tbody>
</table>

Gro√üartig, wir sehen, dass die Zeilen repliziert wurden, wobei die `comments`-Spalte die einzelnen Kommentare enth√§lt! Nachdem wir mit Pandas fertig sind, k√∂nnen wir schnell wieder zu einem `Dataset` wechseln, indem wir den `DataFrame` in den Speicher laden:

```py
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})
```

Okay, das hat uns ein paar Tausend Kommentare zum Arbeiten gegeben!


<Tip>

‚úèÔ∏è **Probier es aus!** Versuche, die `comments`-Spalte von `issues_dataset` mit `Dataset.map()` zu explodieren, _ohne_ auf Pandas zur√ºckzugreifen. Das ist etwas knifflig; der Abschnitt ["Batch Mapping"](https://huggingface.co/docs/datasets/about_map_batch#batch-mapping) der ü§ó Datasets-Dokumentation k√∂nnte f√ºr diese Aufgabe n√ºtzlich sein.

</Tip>

Nachdem wir nun einen Kommentar pro Zeile haben, erstellen wir eine neue Spalte `comments_length`, die die Anzahl der W√∂rter pro Kommentar enth√§lt:

```py
comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)
```

Wir k√∂nnen diese neue Spalte verwenden, um kurze Kommentare herauszufiltern, die typischerweise Dinge wie "cc @lewtun" oder "Danke!" enthalten, die f√ºr unsere Suchmaschine nicht relevant sind. Es gibt keine genaue Zahl, die f√ºr den Filter ausgew√§hlt werden muss, aber etwa 15 W√∂rter scheinen ein guter Anfang zu sein:

```py
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python out
Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})
```

Nachdem wir unseren Datensatz etwas bereinigt haben, verketten wir den Issue-Titel, die Beschreibung und die Kommentare in einer neuen Spalte `text`. Wie √ºblich schreiben wir eine einfache Funktion, die wir an `Dataset.map()` √ºbergeben k√∂nnen:

```py
def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \n "
        + examples["body"]
        + " \n "
        + examples["comments"]
    }

comments_dataset = comments_dataset.map(concatenate_text)
```

Wir sind endlich bereit, einige Embeddings zu erstellen! Schauen wir uns das an.

## Text-Embeddings erstellen[[text-embeddings-erstellen]]

Wir haben in [Kapitel 2](/course/chapter2) gesehen, dass wir Token-Embeddings mit der `AutoModel`-Klasse erhalten k√∂nnen. Alles, was wir tun m√ºssen, ist einen geeigneten Checkpoint auszuw√§hlen, von dem das Modell geladen werden soll. Gl√ºcklicherweise gibt es eine Bibliothek namens `sentence-transformers`, die sich der Erstellung von Embeddings widmet. Wie in der [Dokumentation](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) der Bibliothek beschrieben, ist unser Anwendungsfall ein Beispiel f√ºr _asymmetrische semantische Suche_, da wir eine kurze Abfrage haben, deren Antwort wir in einem l√§ngeren Dokument, wie einem Issue-Kommentar, finden m√∂chten. Die praktische [Modell√ºbersichtstabelle](https://www.sbert.net/docs/pretrained_models.html#model-overview) in der Dokumentation zeigt, dass der Checkpoint `multi-qa-mpnet-base-dot-v1` die beste Leistung f√ºr die semantische Suche aufweist, also werden wir diesen f√ºr unsere Anwendung verwenden. Wir laden auch den Tokenizer mit demselben Checkpoint:

{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, AutoModel
import torch

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)

# Modell auf GPU verschieben, falls verf√ºgbar
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
```

{:else}

```py
from transformers import AutoTokenizer, TFAutoModel
import tensorflow as tf

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True) # from_pt=True l√§dt PyTorch-Gewichte
```

{/if}

Als N√§chstes definieren wir eine Funktion, die die Embeddings f√ºr einen Text-Batch berechnet. Eine g√§ngige Methode zur Erstellung von Satz-Embeddings besteht darin, ein sogenanntes _Pooling_ √ºber die letzten versteckten Zust√§nde des Modells durchzuf√ºhren. Viele Pooling-Strategien sind m√∂glich; eine g√§ngige ist es, die versteckten Zust√§nde mithilfe der Aufmerksamkeitsmaske als Gewichtung _CLS-Pooling_ durchzuf√ºhren. In unserem Fall werden wir einfach den Durchschnitt der versteckten Zust√§nde bilden:

{#if fw === 'pt'}

```py
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]

def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

Testen wir diese Funktion mit dem ersten Text in unserem Korpus:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
torch.Size([1, 768])
```

{:else}

```py
def cls_pooling(outputs):
    return outputs.last_hidden_state[:, 0]

def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    outputs = model(encoded_input)
    return cls_pooling(outputs)
```

Testen wir diese Funktion mit dem ersten Text in unserem Korpus:

```py
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python out
TensorShape([1, 768])
```

{/if}

Das sieht richtig aus -- wir haben einen 768-dimensionalen Vektor f√ºr jeden Text erhalten! Wenden wir nun unsere `get_embeddings()`-Funktion auf den gesamten Datensatz mit `Dataset.map()` an. Wir f√ºgen eine neue Spalte `embeddings` hinzu, die die Vektoren enth√§lt:

```py
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

## FAISS zur effizienten √Ñhnlichkeitssuche verwenden[[faiss-zur-effizienten-√§hnlichkeitssuche-verwenden]]

Jetzt, da wir einen Datensatz mit Embeddings f√ºr alle unsere Issues und Kommentare haben, brauchen wir eine M√∂glichkeit, diese zu durchsuchen. Wir k√∂nnten einfach die Kosinus-√Ñhnlichkeit zwischen einer Abfrage und jedem Embedding im Korpus berechnen, aber dies w√§re sehr langsam, wenn wir Millionen oder Milliarden von Dokumenten h√§tten.

Eine viel schnellere Methode basiert auf _Approximate Nearest Neighbor_-(ANN-)Suche, die versucht, einen kleinen Kompromiss bei der Genauigkeit der Suche zugunsten einer massiven Beschleunigung einzugehen. In ü§ó Datasets haben wir Unterst√ºtzung f√ºr effiziente ANN-Indizes √ºber die [FAISS](https://github.com/facebookresearch/faiss)-Bibliothek (ausgesprochen "face") integriert. FAISS bietet verschiedene Indizierungsmethoden, die die Suche √ºber Millionen oder sogar Milliarden von Vektoren erm√∂glichen.

Wir k√∂nnen FAISS verwenden, indem wir die Methode `Dataset.add_faiss_index()` aufrufen. Diese Methode f√ºgt dem Datensatz einen Index hinzu, der auf den Vektoren in einer bestimmten Spalte basiert:

```py
embeddings_dataset.add_faiss_index(column="embeddings")
```

Sobald der Index erstellt ist, k√∂nnen wir eine Suche durchf√ºhren, indem wir die Methode `Dataset.get_nearest_examples()` verwenden. Diese Methode nimmt eine Abfrage-Embedding und die Anzahl der Beispiele entgegen, die zur√ºckgegeben werden sollen. Wir k√∂nnen unsere `get_embeddings()`-Funktion verwenden, um die Abfrage zu vektorisieren, bevor wir sie an `Dataset.get_nearest_examples()` √ºbergeben. Probieren wir dies mit einer Beispielabfrage aus:

```py
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python out
(1, 768)
```

Jetzt m√ºssen wir dieses Abfrage-Embedding nur noch an `Dataset.get_nearest_examples()` √ºbergeben, um die 5 √§hnlichsten Beispiele zu finden:

```py
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

Die Methode `Dataset.get_nearest_examples()` gibt ein Tupel aus Scores und entsprechenden Beispielen zur√ºck (als Dictionary). Sammeln wir diese in einem `DataFrame`, damit wir die Ergebnisse untersuchen k√∂nnen:

```py
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

Drucken wir nun den Text und die URLs der √§hnlichsten Beispiele aus:

```py
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python out
COMMENT: Requiring online connection is a severe barrier for our users...
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders...
SCORE: 24.555540084838867
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once...
SCORE: 24.148956298828125
TITLE: Add option to reload datasets script each time load_dataset is called
URL: https://github.com/huggingface/datasets/pull/1038
==================================================

COMMENT: I understand the reasons for the offline setting...
SCORE: 23.652145385742188
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: You can set the environment variable `HF_DATASETS_OFFLINE=1` ...
SCORE: 23.447189331054688
TITLE: Is there a way to use load_metric("rouge") offline ?
URL: https://github.com/huggingface/datasets/issues/1055
==================================================
```

Gro√üartig -- die obersten Suchergebnisse beziehen sich alle auf das Offline-Laden von Datens√§tzen! Du kannst diese Technik jetzt anwenden, um deine eigenen semantischen Suchanwendungen zu erstellen!

<Tip>

‚úèÔ∏è **Probier es aus!** Experimentiere mit verschiedenen Abfragen, um zu sehen, welche Arten von Fragen die Suchmaschine beantworten kann. Wie w√ºrdest du die Suchmaschine verbessern? Zum Beispiel k√∂nntest du versuchen, einen anderen Satz von Embeddings zu verwenden, um den Text darzustellen, wie z. B. die Ausgabe des `hidden_state`-Poolings. Alternativ k√∂nntest du den Text weiter bereinigen, indem du Markdown- und Code-Snippets entfernst.

</Tip>

Damit ist unser Ausflug in die fortgeschrittene Nutzung von ü§ó Datasets abgeschlossen. Im n√§chsten Kapitel werden wir mehr √ºber Tokenizer erfahren, insbesondere wie man sie trainiert und mit ihnen Datens√§tze tokenisiert, die zu gro√ü sind, um in den Speicher zu passen. 