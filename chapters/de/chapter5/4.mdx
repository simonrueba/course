# Big Data? ü§ó Datasets zur Rettung![[big-data-datasets-zur-rettung]]

<CourseFloatingBanner chapter={5}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"},
]} />

Heutzutage ist es nicht ungew√∂hnlich, mit Multi-Gigabyte-Datens√§tzen zu arbeiten, besonders wenn du planst, einen Transformer wie BERT oder GPT-2 von Grund auf vorzutrainieren. In diesen F√§llen kann sogar das _Laden_ der Daten eine Herausforderung sein. Zum Beispiel besteht der WebText-Korpus, der zum Vortrainieren von GPT-2 verwendet wurde, aus √ºber 8 Millionen Dokumenten und 40 GB Text ‚Äì das Laden in den Arbeitsspeicher deines Laptops w√ºrde ihm wahrscheinlich einen Herzinfarkt bescheren!

Gl√ºcklicherweise wurde ü§ó Datasets entwickelt, um diese Einschr√§nkungen zu √ºberwinden. Es befreit dich von Speicherverwaltungsproblemen, indem es Datens√§tze als _memory-mapped_ Dateien behandelt, und von Festplattenlimits durch das _Streaming_ der Eintr√§ge in einem Korpus.

<Youtube id="JwISwTCPPWo"/>

In diesem Abschnitt werden wir diese Funktionen von ü§ó Datasets mit einem riesigen 825 GB Korpus namens [The Pile](https://pile.eleuther.ai) untersuchen. Los geht's!

## Was ist The Pile?[[was-ist-the-pile]]

The Pile ist ein englischer Textkorpus, der von [EleutherAI](https://www.eleuther.ai) f√ºr das Training gro√üer Sprachmodelle erstellt wurde. Er umfasst eine vielf√§ltige Palette von Datens√§tzen, die wissenschaftliche Artikel, GitHub-Code-Repositories und gefilterten Webtext umfassen. Der Trainingskorpus ist in [14 GB Chunks](https://the-eye.eu/public/AI/pile/) verf√ºgbar, und du kannst auch mehrere der [einzelnen Komponenten](https://the-eye.eu/public/AI/pile_preliminary_components/) herunterladen. Werfen wir zun√§chst einen Blick auf den PubMed Abstracts-Datensatz, einen Korpus von Abstracts aus 15 Millionen biomedizinischen Publikationen auf [PubMed](https://pubmed.ncbi.nlm.nih.gov/). Der Datensatz liegt im [JSON Lines-Format](https://jsonlines.org) vor und ist mit der `zstandard`-Bibliothek komprimiert. Zuerst m√ºssen wir diese installieren:

```py
!pip install zstandard
```

Als N√§chstes k√∂nnen wir den Datensatz mit der Methode f√ºr entfernte Dateien laden, die wir in [Abschnitt 2](/course/chapter5/2) gelernt haben:

```py
from datasets import load_dataset

# Dies dauert ein paar Minuten, also hol dir einen Tee oder Kaffee, w√§hrend du wartest :)
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python out
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

Wir sehen, dass unser Datensatz 15.518.009 Zeilen und 2 Spalten hat ‚Äì das ist eine Menge!

<Tip>

‚úé Standardm√§√üig dekomprimiert ü§ó Datasets die Dateien, die zum Laden eines Datensatzes ben√∂tigt werden. Wenn du Festplattenspeicher sparen m√∂chtest, kannst du `DownloadConfig(delete_extracted=True)` an das `download_config`-Argument von `load_dataset()` √ºbergeben. Weitere Details findest du in der [Dokumentation](https://huggingface.co/docs/datasets/package_reference/builder_classes#datasets.DownloadConfig).

</Tip>

Schauen wir uns den Inhalt des ersten Beispiels an:

```py
pubmed_dataset[0]
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Okay, das sieht wie der Abstract eines medizinischen Artikels aus. Sehen wir uns nun an, wie viel RAM wir zum Laden des Datensatzes verwendet haben!

## Die Magie des Memory Mappings[[die-magie-des-memory-mappings]]

Eine einfache M√∂glichkeit, die Speichernutzung in Python zu messen, ist die [`psutil`](https://psutil.readthedocs.io/en/latest/)-Bibliothek, die wie folgt mit `pip` installiert werden kann:

```python
!pip install psutil
```

Sie bietet eine `Process`-Klasse, mit der wir die Speichernutzung des aktuellen Prozesses wie folgt √ºberpr√ºfen k√∂nnen:

```py
import psutil

# Process.memory_info wird in Bytes ausgedr√ºckt, also in Megabyte umwandeln
print(f"Verwendeter RAM: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python out
Verwendeter RAM: 5678.33 MB
```

Hier bezieht sich das `rss`-Attribut auf die _Resident Set Size_, also den Anteil des Speichers, den ein Prozess im RAM belegt. Diese Messung beinhaltet auch den Speicher, der vom Python-Interpreter und den geladenen Bibliotheken verwendet wird, sodass die tats√§chliche Menge an Speicher, die zum Laden des Datensatzes verwendet wird, etwas kleiner ist. Zum Vergleich sehen wir uns an, wie gro√ü der Datensatz auf der Festplatte ist, indem wir das Attribut `dataset_size` verwenden. Da das Ergebnis wie zuvor in Bytes ausgedr√ºckt wird, m√ºssen wir es manuell in Gigabyte umwandeln:

```py
print(f"Datensatzgr√∂√üe in Bytes: {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Datensatzgr√∂√üe (Cache-Datei): {size_gb:.2f} GB")
```

```python out
Datensatzgr√∂√üe in Bytes: 20979437051
Datensatzgr√∂√üe (Cache-Datei): 19.54 GB
```

Sch√∂n ‚Äì obwohl er fast 20 GB gro√ü ist, k√∂nnen wir den Datensatz mit viel weniger RAM laden und darauf zugreifen!

<Tip>

‚úèÔ∏è **Probier es aus!** W√§hle eines der [Teilmengen](https://the-eye.eu/public/AI/pile_preliminary_components/) aus The Pile, das gr√∂√üer ist als der RAM deines Laptops oder Desktops, lade es mit ü§ó Datasets und miss die Menge des verwendeten RAMs. Beachte, dass du dies in einem neuen Prozess tun solltest, um eine genaue Messung zu erhalten. Die dekomprimierten Gr√∂√üen jeder Teilmenge findest du in Tabelle 1 des [Pile-Papiers](https://arxiv.org/abs/2101.00027).

</Tip>

Wenn du mit Pandas vertraut bist, mag dieses Ergebnis √ºberraschen, aufgrund von Wes Kinneys ber√ºhmter [Faustregel](https://wesmckinney.com/blog/apache-arrow-pandas-internals/), dass du typischerweise 5 bis 10 Mal so viel RAM ben√∂tigst wie die Gr√∂√üe deines Datensatzes. Wie l√∂st ü§ó Datasets also dieses Speicherverwaltungsproblem? ü§ó Datasets behandelt jeden Datensatz als [Memory-Mapped File](https://de.wikipedia.org/wiki/Memory-Mapped_File), was eine Zuordnung zwischen RAM und Dateisystemspeicher bereitstellt. Dies erm√∂glicht es der Bibliothek, auf Elemente des Datensatzes zuzugreifen und damit zu arbeiten, ohne ihn vollst√§ndig in den Speicher laden zu m√ºssen.

Memory-Mapped Files k√∂nnen auch √ºber mehrere Prozesse hinweg gemeinsam genutzt werden, was es erm√∂glicht, Methoden wie `Dataset.map()` zu parallelisieren, ohne den Datensatz verschieben oder kopieren zu m√ºssen. Unter der Haube werden all diese F√§higkeiten durch das [Apache Arrow](https://arrow.apache.org)-Speicherformat und die [`pyarrow`](https://arrow.apache.org/docs/python/index.html)-Bibliothek realisiert, die das Laden und Verarbeiten von Daten blitzschnell machen. (Weitere Details zu Apache Arrow und Vergleiche mit Pandas findest du im [Blogbeitrag von Dejan Simic](https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a).) Um dies in Aktion zu sehen, f√ºhren wir einen kleinen Geschwindigkeitstest durch, indem wir √ºber alle Elemente im PubMed Abstracts-Datensatz iterieren:

```py
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iteriert √ºber {len(pubmed_dataset)} Beispiele (ca. {size_gb:.1f} GB) in "
    f"{time:.1f}s, d.h. {size_gb/time:.3f} GB/s"
)
```

```python out
'Iteriert √ºber 15518009 Beispiele (ca. 19.5 GB) in 64.2s, d.h. 0.304 GB/s'
```

Hier haben wir das `timeit`-Modul von Python verwendet, um die Ausf√ºhrungszeit von `code_snippet` zu messen. Du wirst normalerweise in der Lage sein, √ºber einen Datensatz mit einer Geschwindigkeit von einigen Zehnteln GB/s bis zu mehreren GB/s zu iterieren. Dies funktioniert hervorragend f√ºr die √ºberwiegende Mehrheit der Anwendungen, aber manchmal musst du mit einem Datensatz arbeiten, der zu gro√ü ist, um ihn √ºberhaupt auf der Festplatte deines Laptops zu speichern. Wenn wir zum Beispiel versuchen w√ºrden, The Pile vollst√§ndig herunterzuladen, br√§uchten wir 825 GB freien Speicherplatz! Um diese F√§lle zu behandeln, bietet ü§ó Datasets eine Streaming-Funktion, mit der wir Elemente on-the-fly herunterladen und darauf zugreifen k√∂nnen, ohne den gesamten Datensatz herunterladen zu m√ºssen. Schauen wir uns an, wie das funktioniert.

<Tip>

üí° In Jupyter Notebooks kannst du Zellen auch mit der [`%%timeit` Magic-Funktion](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit) timen.

</Tip>

## Streaming von Datens√§tzen[[streaming-von-datens√§tzen]]

Um das Streaming von Datens√§tzen zu aktivieren, musst du nur das Argument `streaming=True` an die Funktion `load_dataset()` √ºbergeben. Laden wir zum Beispiel den PubMed Abstracts-Datensatz erneut, aber im Streaming-Modus:

```py
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

Anstelle des bekannten `Dataset`, dem wir an anderer Stelle in diesem Kapitel begegnet sind, ist das mit `streaming=True` zur√ºckgegebene Objekt ein `IterableDataset`. Wie der Name schon sagt, m√ºssen wir √ºber ein `IterableDataset` iterieren, um auf seine Elemente zuzugreifen. Wir k√∂nnen auf das erste Element unseres gestreamten Datensatzes wie folgt zugreifen:

```py
next(iter(pubmed_dataset_streamed))
```

```python out
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

Die Elemente aus einem gestreamten Datensatz k√∂nnen on-the-fly mit `IterableDataset.map()` verarbeitet werden, was w√§hrend des Trainings n√ºtzlich ist, wenn du die Eingaben tokenisieren musst. Der Prozess ist genau derselbe wie der, den wir zum Tokenisieren unseres Datensatzes in [Kapitel 3](/course/chapter3) verwendet haben, mit dem einzigen Unterschied, dass die Ausgaben einzeln zur√ºckgegeben werden:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python out
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

<Tip>

üí° Um die Tokenisierung mit Streaming zu beschleunigen, kannst du `batched=True` √ºbergeben, wie wir im letzten Abschnitt gesehen haben. Es verarbeitet die Beispiele Batch f√ºr Batch; die Standard-Batch-Gr√∂√üe ist 1.000 und kann mit dem `batch_size`-Argument angegeben werden.

</Tip>

Du kannst einen gestreamten Datensatz auch mit `IterableDataset.shuffle()` mischen, aber im Gegensatz zu `Dataset.shuffle()` mischt dies nur die Elemente in einer vordefinierten `buffer_size`:

```py
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python out
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

In diesem Beispiel haben wir ein zuf√§lliges Beispiel aus den ersten 10.000 Beispielen im Puffer ausgew√§hlt. Sobald auf ein Beispiel zugegriffen wird, wird sein Platz im Puffer mit dem n√§chsten Beispiel im Korpus gef√ºllt (d.h. dem 10.001. Beispiel im obigen Fall). Du kannst auch Elemente aus einem gestreamten Datensatz mit den Funktionen `IterableDataset.take()` und `IterableDataset.skip()` ausw√§hlen, die √§hnlich wie `Dataset.select()` funktionieren. Um beispielsweise die ersten 5 Beispiele im PubMed Abstracts-Datensatz auszuw√§hlen, k√∂nnen wir Folgendes tun:

```py
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

Ebenso kannst du die Funktion `IterableDataset.skip()` verwenden, um Trainings- und Validierungs-Splits aus einem gemischten Datensatz wie folgt zu erstellen:

```py
# √úberspringe die ersten 1.000 Beispiele und nimm den Rest in den Trainingssatz auf
train_dataset = shuffled_dataset.skip(1000)
# Nimm die ersten 1.000 Beispiele f√ºr den Validierungssatz
validation_dataset = shuffled_dataset.take(1000)
```

Runden wir unsere Erkundung des Datensatz-Streamings mit einer g√§ngigen Anwendung ab: dem Kombinieren mehrerer Datens√§tze zu einem einzigen Korpus. ü§ó Datasets bietet eine `interleave_datasets()`-Funktion, die eine Liste von `IterableDataset`-Objekten in ein einzelnes `IterableDataset` umwandelt, wobei die Elemente des neuen Datensatzes durch Abwechseln zwischen den Quellenbeispielen erhalten werden. Diese Funktion ist besonders n√ºtzlich, wenn du versuchst, gro√üe Datens√§tze zu kombinieren. Als Beispiel streamen wir die FreeLaw-Teilmenge von The Pile, einen 51 GB gro√üen Datensatz mit Rechtsgutachten von US-Gerichten:

```py
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python out
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

Dieser Datensatz ist gro√ü genug, um den RAM der meisten Laptops zu belasten, dennoch konnten wir ihn problemlos laden und darauf zugreifen! Kombinieren wir nun die Beispiele aus den FreeLaw- und PubMed Abstracts-Datens√§tzen mit der Funktion `interleave_datasets()`:

```py
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python out
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

Wie du siehst, wechselt der `combined_dataset` zwischen den beiden Quelldatens√§tzen. Standardm√§√üig durchl√§uft `interleave_datasets()` die einzelnen Datens√§tze vollst√§ndig, bevor es zum n√§chsten √ºbergeht, aber du kannst das Verhalten mit dem `probabilities`-Argument √§ndern, um die Datens√§tze basierend auf einer benutzerdefinierten Stichprobenverteilung zu mischen.

Damit schlie√üen wir unsere Tour durch die ü§ó Datasets-Bibliothek ab. Du solltest jetzt ein gutes Verst√§ndnis f√ºr die Hauptfunktionen haben und bereit sein, sie auf deine eigenen Datens√§tze anzuwenden! 