# EinfÃ¼hrung[[einfÃ¼hrung]]

<CourseFloatingBanner
    chapter={5}
    classNames="absolute z-10 right-0 top-0"
/>

In [Kapitel 3](/course/chapter3) hast du einen ersten Einblick in die ğŸ¤— Datasets-Bibliothek erhalten und gesehen, dass es drei Hauptschritte beim Fine-Tuning eines Modells gibt:

1. Laden eines Datensatzes vom Hugging Face Hub.
2. Vorverarbeitung der Daten mit `Dataset.map()`.
3. Laden und Berechnen von Metriken.

Aber das kratzt nur an der OberflÃ¤che dessen, was ğŸ¤— Datasets leisten kann! In diesem Kapitel werden wir uns eingehend mit der Bibliothek beschÃ¤ftigen. Unterwegs finden wir Antworten auf die folgenden Fragen:

* Was tust du, wenn dein Datensatz nicht auf dem Hub verfÃ¼gbar ist?
* Wie kannst du einen Datensatz zerlegen und analysieren? (Und was, wenn du _wirklich_ Pandas verwenden musst?)
* Was tust du, wenn dein Datensatz riesig ist und den Arbeitsspeicher deines Laptops zum Schmelzen bringen wÃ¼rde?
* Was zum Teufel sind "Memory Mapping" und Apache Arrow?
* Wie kannst du deinen eigenen Datensatz erstellen und auf den Hub hochladen?

Die Techniken, die du hier lernst, bereiten dich auf die fortgeschrittenen Tokenisierungs- und Fine-Tuning-Aufgaben in [Kapitel 6](/course/chapter6) und [Kapitel 7](/course/chapter7) vor -- also schnapp dir einen Kaffee und legen wir los! 